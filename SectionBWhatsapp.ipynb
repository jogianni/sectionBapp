{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SectionBWhatsapp.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOl4zsuRs6xh06YyUuX3M+H",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jogianni/sectionBapp/blob/main/SectionBWhatsapp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OdXSM4-IecVo"
      },
      "source": [
        "**IMPORTS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBmZgymlX0gt"
      },
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import io\n",
        "import os\n",
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "import numpy as np\n",
        "import datetime\n",
        "!pip install emoji --upgrade\n",
        "import emoji\n",
        "from collections import Counter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ezzrpA65fbA"
      },
      "source": [
        "**Load - Mount GDrive and Read-In**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJUZEvNOX2C_"
      },
      "source": [
        "#mount to gdrive - you will be asked to sign into \n",
        "drive.mount('/content/gdrive')\n",
        "root_path = 'gdrive/My Drive/IESE/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GEELl7ExX3hR"
      },
      "source": [
        "#os.listdir('/content/gdrive/My Drive/IESE')\n",
        "os.chdir('/content/gdrive/My Drive/IESE')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BpHTIKCqc_SJ"
      },
      "source": [
        "def read_file(file):\n",
        "    '''Reads Whatsapp text file into a list of strings'''\n",
        "    x = open(file,'r', encoding = 'utf-8') #Opens the text file into variable x but the variable cannot be explored yet\n",
        "    y = x.read() #By now it becomes a huge chunk of string that we need to separate line by line\n",
        "    content = y.splitlines() #The splitline method converts the chunk of string into a list of strings\n",
        "    return content\n",
        "\n",
        "chat = read_file('sectionB10.24.2021.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DvViU02-c2_M"
      },
      "source": [
        "**Transform - Add some auxiliary data as well as parse dates and do light analysis**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9k1xOy6YMFKW"
      },
      "source": [
        "#Drop lines that don't start with '[' - the class tends to send lots of lists with carriage returns. We don't want to count thoe as individual messages and it's hard to identify their owner, so we drop them\n",
        "chat = [x for x in chat if x.startswith('[')]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PR6mBJ4fYNCT"
      },
      "source": [
        "#reads date in time if your phone is set to miliatary time\n",
        "#if you phone is am/pm d{2}.{3} is needed to replace the last d{2} in time \n",
        "regex = re.compile(r'\\[(?P<date>\\d{1,2}\\/\\d{1,2}\\/\\d{2})\\s(?P<time>\\d{1,2}:\\d{2}:\\d{2})]\\s(?P<Name>[^:]*):\\s(?P<content>.+|\\n+(?!)\\[\\d{2}\\/\\d{2}\\/\\d{4})')\n",
        "\n",
        "#print the before and after chat count, since the chat list action eliminates lines that don't comply with the regex - we are only interested in timestamped actions anyways\n",
        "print (len(chat))\n",
        "chat_matches = [regex.search(content) for content in chat]\n",
        "chat_list = [m.groupdict() for m in chat_matches if not m is None]\n",
        "print (len(chat_list))\n",
        "#print (chat_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4ii7ScxUq3D"
      },
      "source": [
        "#reads date in time if your phone is set to am pm time\n",
        "\n",
        "#regex = re.compile(r'\\[(?P<date>\\d{1,2}\\/\\d{1,2}\\/\\d{2})\\s(?P<time>\\d{1,2}:\\d{2}:\\d{2}.{3})]\\s(?P<Name>[^:]*):\\s(?P<content>.+|\\n+(?!)\\[\\d{2}\\/\\d{2}\\/\\d{4})')\n",
        "\n",
        "#print (len(chat))\n",
        "#chat_matches = [regex.search(content) for content in chat]\n",
        "#chat_list = [m.groupdict() for m in chat_matches if not m is None]\n",
        "#print (len(chat_list))\n",
        "#print (chat_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPKDm7v9WaA4"
      },
      "source": [
        "#make Dataframe\n",
        "df = pd.DataFrame(chat_list)\n",
        "\n",
        "df['datetime'] = pd.to_datetime(df['date'] + ' ' + df['time'])\n",
        "\n",
        "df['date'] = [datetime.datetime.strptime(x, '%d/%m/%y') for x in df['date']]\n",
        "\n",
        "#add some datums\n",
        "df['msg_len']  = df['content'].str.len()\n",
        "#Get word and message count\n",
        "df['Letter_Count'] = df['content'].apply(lambda s : len(s.replace(' ','')))\n",
        "df['Word_Count'] = df['content'].apply(lambda s : len(s.split(' ')))\n",
        "#letter and word totals\n",
        "df['Letter_Count'].sum(), df['Word_Count'].sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5LcAcvRZm1U4"
      },
      "source": [
        "#Read-in Auciliary data - I took in my classmates' team and country of origin to give further analytics\n",
        "coa_df = pd.read_csv('COA.csv') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCYo29hTmz7U"
      },
      "source": [
        "#tack on auxiliary data \n",
        "merged_df = pd.merge(left=df, right=coa_df, how='left', left_on='Name', right_on='Name')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZY3RK_JLdAk6"
      },
      "source": [
        "#output basid data before doing some emoji analysis - pull name from input file so they're distinguishable \n",
        "merged_df.to_csv(\"sectionB10.24.2021_converted.csv\",index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnDYOp8MfYLW"
      },
      "source": [
        "**Emoji analysis** - Here we use the emoji library to parse emojis in the chat. We will take some counts and output into a large table of every emoji, so we can use properly in Tableau\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6NALllBm7Sa"
      },
      "source": [
        "#use emoji package to decode emoji output\n",
        "def extract_emojis(str):\n",
        "    return ''.join(c for c in str if c in emoji.UNICODE_EMOJI)\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bk2FnLZp0H64"
      },
      "source": [
        "#create emoji counts \n",
        "def Count_Emojis(df, only_one_per_message = False):\n",
        "    series = df['content']\n",
        "    all_words = \"\"\n",
        "    for sentence in series:\n",
        "        all_words += extract_emojis(sentence)\n",
        "    word_count = Counter(all_words)\n",
        "    \n",
        "    \n",
        "    ordered = {}\n",
        "    \n",
        "    for key, number in word_count.most_common()[:50]:\n",
        "        ordered[key] = []\n",
        "    \n",
        "    for sentence in series:\n",
        "        sentence_count = Counter(extract_emojis(sentence))\n",
        "        \n",
        "        for word in ordered:\n",
        "            if only_one_per_message:\n",
        "                count = 1 if sentence_count[word] else 0\n",
        "            else:\n",
        "                count = sentence_count[word] if sentence_count[word] else 0\n",
        "            ordered[word] += [count] \n",
        "             \n",
        "            \n",
        "    ordered['content'] = list(series)\n",
        "    ordered['date'] = list(df['date'])\n",
        "    ordered['Name'] = list(df['Name'])\n",
        "    ordered['Country of Origin'] = list(df['Country of Origin'])\n",
        "    ordered['Diagnosed Country'] = list(df['Diagnosed Country'])\n",
        "    ordered['Team'] = list(df['Team'])\n",
        "    \n",
        "    return pd.DataFrame(ordered)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZEBb0r0x081D"
      },
      "source": [
        "#add on previously ingested demograpic data\n",
        "emojis_counts = Count_Emojis(merged_df)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UOxdESuoYBM"
      },
      "source": [
        "#output\n",
        "emojis_counts.to_csv(\"sectionB10.24.2021_converted_emoji.csv\",index=False)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxeTLN-l4uWj"
      },
      "source": [
        "**Crush Metric** - who is talkative around who? Within the section chat we look at who talks the most within a 2 minute window after someone else speaks? In tableau we will go on to measure vs their average. So if suzy usually sends 1 message per day but will send 5 if Johnny talks maybe she likes to see his name?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WsITIeKkdbv8"
      },
      "source": [
        "#get top 3 repliers function\n",
        "def get_most_replied_users(replies_dict):\n",
        "    most_replied ='|'.join(sorted(replies_dict, key=lambda key: replies_dict[key],reverse=True)[:3])\n",
        "    return most_replied"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bi97ucGjme7-"
      },
      "source": [
        "#create counter function\n",
        "def count_user_replies(df):\n",
        "    reply_duration_minutes = 2 #minutes\n",
        "    users_replied_to_user = {}\n",
        "    #iterate through messages\n",
        "    for index, row in df.iterrows():\n",
        "        d = row['datetime'] #current messge datetime\n",
        "        user = row['Name']\n",
        "        #if user is not in the dictionary add it\n",
        "        if not user in users_replied_to_user:\n",
        "            users_replied_to_user[user] = []\n",
        "        \n",
        "        #find the messages within a time window after the current message datetime\n",
        "        user_replies = df[(df['datetime'] > d) & (df['datetime'] < d+datetime.timedelta(minutes=reply_duration_minutes))]['Name']\n",
        "        users_replied_to_user[user]+=list(set(user_replies)) #only non-duplicates\n",
        "\n",
        "    #use Counter to count frequency of replies\n",
        "    for k,v in users_replied_to_user.items():\n",
        "        users_replied_to_user[k] = Counter(v)\n",
        "    \n",
        "    return users_replied_to_user"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3XHPuk6Ndtmc"
      },
      "source": [
        "#build dataframe\n",
        "replies =   count_user_replies(df)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ysDojt1Lp6Xy"
      },
      "source": [
        "#reshape the \n",
        "replies_reshaped =[]\n",
        "for k,v in replies.items():\n",
        "    replies_reshaped.append([k, get_most_replied_users(v)])\n",
        "\n",
        "replies_reshaped_df = pd.DataFrame(replies_reshaped)\n",
        "\n",
        "#print(replies_reshaped_df)\n",
        "\n"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yix3hvClK2O-"
      },
      "source": [
        "**Output**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvhxEE74eDR7"
      },
      "source": [
        "#output to csva\n",
        "replies_reshaped_df.to_csv(\"sectionB10.24.2021_replies_df.csv\",index=False)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Enh2wboHItVq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}